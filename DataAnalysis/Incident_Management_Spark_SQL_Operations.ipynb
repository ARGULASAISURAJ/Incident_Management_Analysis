{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('IMSQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('incident_event_log.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('resolved_ts',to_timestamp(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('opened_ts',to_timestamp(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('sys_created_ts',to_timestamp(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('sys_updated_ts',to_timestamp(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('closed_ts',to_timestamp(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('resolved',to_date(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('opened',to_date(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('sys_created',to_date(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('sys_updated',to_date(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('closed',to_date(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "                withColumn('duration',datediff(to_date(df.resolved_at, 'dd/MM/yyyy HH:mm'),to_date(df.opened_at, 'dd/MM/yyyy HH:mm')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_incidents=df.filter(\"incident_state=='Closed'\").sort(\"sys_mod_count\",ascending=False).dropDuplicates([\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_incidents.createOrReplaceTempView(\"IM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|              number|   string|   null|\n",
      "|      incident_state|   string|   null|\n",
      "|              active|  boolean|   null|\n",
      "|  reassignment_count|      int|   null|\n",
      "|        reopen_count|      int|   null|\n",
      "|       sys_mod_count|      int|   null|\n",
      "|            made_sla|  boolean|   null|\n",
      "|           caller_id|   string|   null|\n",
      "|           opened_by|   string|   null|\n",
      "|           opened_at|   string|   null|\n",
      "|      sys_created_by|   string|   null|\n",
      "|      sys_created_at|   string|   null|\n",
      "|      sys_updated_by|   string|   null|\n",
      "|      sys_updated_at|   string|   null|\n",
      "|        contact_type|   string|   null|\n",
      "|            location|   string|   null|\n",
      "|            category|   string|   null|\n",
      "|         subcategory|   string|   null|\n",
      "|           u_symptom|   string|   null|\n",
      "|             cmdb_ci|   string|   null|\n",
      "|              impact|   string|   null|\n",
      "|             urgency|   string|   null|\n",
      "|            priority|   string|   null|\n",
      "|    assignment_group|   string|   null|\n",
      "|         assigned_to|   string|   null|\n",
      "|           knowledge|  boolean|   null|\n",
      "|u_priority_confir...|  boolean|   null|\n",
      "|              notify|   string|   null|\n",
      "|          problem_id|   string|   null|\n",
      "|                 rfc|   string|   null|\n",
      "|              vendor|   string|   null|\n",
      "|           caused_by|   string|   null|\n",
      "|         closed_code|   string|   null|\n",
      "|         resolved_by|   string|   null|\n",
      "|         resolved_at|   string|   null|\n",
      "|           closed_at|   string|   null|\n",
      "|         resolved_ts|timestamp|   null|\n",
      "|           opened_ts|timestamp|   null|\n",
      "|      sys_created_ts|timestamp|   null|\n",
      "|      sys_updated_ts|timestamp|   null|\n",
      "|           closed_ts|timestamp|   null|\n",
      "|            resolved|     date|   null|\n",
      "|              opened|     date|   null|\n",
      "|         sys_created|     date|   null|\n",
      "|         sys_updated|     date|   null|\n",
      "|              closed|     date|   null|\n",
      "|            duration|      int|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe IM\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Top 5 people with most resolved incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    resolved_by|Incidents_Resolved|\n",
      "+---------------+------------------+\n",
      "| Resolved by 11|              3071|\n",
      "| Resolved by 15|              2415|\n",
      "|Resolved by 103|               689|\n",
      "|Resolved by 177|               686|\n",
      "| Resolved by 32|               597|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select resolved_by,count(number) as Incidents_Resolved from IM \\\n",
    "            group by resolved_by order by Incidents_Resolved desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Based on least average duration, find the top 5 people with maxmium number of incidents resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+----------------+\n",
      "|    resolved_by|Incidents_Resolved|Average_Duration|\n",
      "+---------------+------------------+----------------+\n",
      "| Resolved by 10|                 4|             0.0|\n",
      "| Resolved by 94|                 4|             0.0|\n",
      "| Resolved by 26|                 2|             0.0|\n",
      "|Resolved by 145|                 2|             0.0|\n",
      "|Resolved by 219|                 1|             0.0|\n",
      "+---------------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select resolved_by,count(number) as Incidents_Resolved,mean(duration) as Average_Duration from IM \\\n",
    "            group by resolved_by order by Average_Duration asc,Incidents_Resolved desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. People with maximum number of high impact incidents resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    resolved_by|Incidents_Resolved|\n",
      "+---------------+------------------+\n",
      "| Resolved by 98|                20|\n",
      "|Resolved by 137|                17|\n",
      "| Resolved by 11|                15|\n",
      "|Resolved by 165|                13|\n",
      "|Resolved by 111|                12|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select resolved_by,count(number) as Incidents_Resolved from IM \\\n",
    "            where impact = '1 - High' group by resolved_by order by Incidents_Resolved desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. In each impact levels, find the person with most number of incidents resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------------+\n",
      "|    impact|   resolved_by|Incidents_Resolved|\n",
      "+----------+--------------+------------------+\n",
      "|  1 - High|Resolved by 98|                20|\n",
      "|2 - Medium|Resolved by 11|              3045|\n",
      "|   3 - Low|Resolved by 66|               194|\n",
      "+----------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select impact,resolved_by,Incidents_Resolved from (\\\n",
    "            select impact,resolved_by,count(number) as Incidents_Resolved,\\\n",
    "            row_number() over (partition by impact order by count(number) desc) as row_number from IM \\\n",
    "            group by impact,resolved_by order by impact asc, Incidents_Resolved desc) as rows \\\n",
    "            where row_number = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. In each urgency levels, find the person with most number of incidents resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|   urgency|    resolved_by|Incidents_Resolved|\n",
      "+----------+---------------+------------------+\n",
      "|  1 - High|Resolved by 166|                38|\n",
      "|2 - Medium| Resolved by 11|              3047|\n",
      "|   3 - Low| Resolved by 66|               195|\n",
      "+----------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select urgency,resolved_by,Incidents_Resolved from (\\\n",
    "            select urgency,resolved_by,count(number) as Incidents_Resolved,\\\n",
    "            row_number() over (partition by urgency order by count(number) desc) as row_number from IM \\\n",
    "            group by urgency,resolved_by order by urgency asc, Incidents_Resolved desc) as rows \\\n",
    "            where row_number = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. In each priority levels, find the person with most number of incidents resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+------------------+\n",
      "|    priority|    resolved_by|Incidents_Resolved|\n",
      "+------------+---------------+------------------+\n",
      "|1 - Critical| Resolved by 98|                16|\n",
      "|    2 - High|Resolved by 166|                40|\n",
      "|3 - Moderate| Resolved by 11|              3040|\n",
      "|     4 - Low| Resolved by 66|               195|\n",
      "+------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select priority,resolved_by,Incidents_Resolved from (\\\n",
    "            select priority,resolved_by,count(number) as Incidents_Resolved,\\\n",
    "            row_number() over (partition by priority order by count(number) desc) as row_number from IM \\\n",
    "            group by priority,resolved_by order by priority asc, Incidents_Resolved desc) as rows \\\n",
    "            where row_number = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Find each contact type as a percentage of total incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------+\n",
      "|  contact_type|Incidents_Reported|Percentage|\n",
      "+--------------+------------------+----------+\n",
      "|         Phone|             24688|     99.08|\n",
      "|         Email|                59|      0.24|\n",
      "|  Self service|               158|      0.63|\n",
      "|           IVR|                 9|      0.04|\n",
      "|Direct opening|                 4|      0.02|\n",
      "+--------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select contact_type,count(number) as Incidents_Reported,\\\n",
    "            cast(count(number)*100/sum(count(number)) over() as decimal(4,2)) as Percentage \\\n",
    "            from IM group by contact_type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. On each priority level, find the percentage of incidents which made SLA and which did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+---------------+----------+\n",
      "|    priority|made_sla|No_of_Incidents|Percentage|\n",
      "+------------+--------+---------------+----------+\n",
      "|1 - Critical|     YES|              5|      1.85|\n",
      "|1 - Critical|      NO|            265|     98.15|\n",
      "|    2 - High|     YES|              2|      0.49|\n",
      "|    2 - High|      NO|            406|     99.51|\n",
      "|3 - Moderate|     YES|          15145|     64.54|\n",
      "|3 - Moderate|      NO|           8321|     35.46|\n",
      "|     4 - Low|     YES|            651|     84.11|\n",
      "|     4 - Low|      NO|            123|     15.89|\n",
      "+------------+--------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select priority,case when made_sla = 'false' then 'NO' else 'YES' end as made_sla,\\\n",
    "            count(number) as No_of_Incidents,\\\n",
    "            cast(count(number)*100/sum(count(number)) over(partition by priority) as decimal(4,2)) as Percentage \\\n",
    "            from IM group by priority,made_sla order by priority asc, made_sla desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Top 5 location with the maximum number of incidents reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|    location|Incidents_Reported|\n",
      "+------------+------------------+\n",
      "|Location 204|              5554|\n",
      "|Location 161|              4002|\n",
      "|Location 143|              3276|\n",
      "|Location 108|              2140|\n",
      "| Location 93|              1934|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select location,count(number) as Incidents_Reported from IM \\\n",
    "            group by location order by Incidents_reported desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Which category of issues missed meeting the SLA the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------+\n",
      "|   category|No_Of_Incidents_missing_SLA|\n",
      "+-----------+---------------------------+\n",
      "|Category 46|                       1254|\n",
      "|Category 26|                       1017|\n",
      "|Category 53|                       1009|\n",
      "|Category 42|                        689|\n",
      "|Category 23|                        505|\n",
      "+-----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\" select category,count(number) as No_Of_Incidents_missing_SLA from IM \\\n",
    "            where made_sla=false group by category order by No_Of_Incidents_missing_SLA desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
