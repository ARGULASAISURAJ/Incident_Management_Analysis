{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC to predict whether an incident met SLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Incident Management dataset has about 141712 records of 24918 incidents. Each state of the incident is being captured as an individual record with few exceptions where the closed state of an incident is recorded more than once. With the help of the below segment of the code, we load and clean the Incident Management data so that only one record representing the truly closed state per incident is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a spark session and load the Incident Management Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('IMMLSVC').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('incident_event_log.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new timestamp and date columns for all the attributes that had timestamp details stored as string\n",
    "# The target column made_sla is converted to hold numeric values\n",
    "# Two durations (resolved and closed) are calculated to be passed as the independent variables\n",
    "\n",
    "df=df.withColumn('resolved_ts',to_timestamp(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened_ts',to_timestamp(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed_ts',to_timestamp(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('resolved',to_date(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('opened',to_date(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('closed',to_date(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n",
    "        withColumn('knowledge', f.col('knowledge').cast('string')).\\\n",
    "        replace(['TRUE',], 'True', subset='knowledge').\\\n",
    "        replace(['FALSE'], 'False', subset='knowledge').\\\n",
    "        withColumn('resolved_duration',datediff(to_date(df.resolved_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('closed_duration',datediff(to_date(df.closed_at, 'dd/MM/yyyy HH:mm'),\\\n",
    "                                                to_date(df.opened_at, 'dd/MM/yyyy HH:mm'))).\\\n",
    "        withColumn('made_sla_int',df.made_sla.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set has multiple states(New, Active, Awaiting user info, Resolved, Closed etc. ) of an incident. With the help \n",
    "# of the below command, we are just filtering one record per incident, that has the truly closed state of the incident. \n",
    "\n",
    "df_unique_incidents=df.filter(\"incident_state=='Closed'\").sort(\"sys_mod_count\",ascending=False).dropDuplicates([\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the dependent and the independent variables that are identified as most useful attributes to make predictions\n",
    "\n",
    "data=df_unique_incidents.select(['sys_mod_count','opened_by','location','category','priority','assignment_group',\n",
    "                                 'knowledge','resolved_duration','closed_duration','made_sla_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 70-30 train test split\n",
    "\n",
    "train_data,test_data=data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer,StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    "\n",
    "opened_by_indexer = StringIndexer(inputCol='opened_by',outputCol='opened_by_index',handleInvalid='keep')\n",
    "location_indexer = StringIndexer(inputCol='location',outputCol='location_index',handleInvalid='keep')\n",
    "category_indexer = StringIndexer(inputCol='category',outputCol='category_index',handleInvalid='keep')\n",
    "priority_indexer = StringIndexer(inputCol='priority',outputCol='priority_index',handleInvalid='keep')\n",
    "assignment_group_indexer = StringIndexer(inputCol='assignment_group',outputCol='assignment_group_index',handleInvalid='keep')\n",
    "knowledge_indexer = StringIndexer(inputCol='knowledge',outputCol='knowledge_index',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector assembler is used to create a vector of input features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['opened_by_index','location_index','category_index',\n",
    "                                       'priority_index','assignment_group_index','knowledge_index'],\n",
    "                            outputCol=\"unscaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler is used to scale the data for the linear SVC to perform well on the training data\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\",outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object for the Linear SVC model\n",
    "\n",
    "svc_model = LinearSVC(labelCol='made_sla_int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data\n",
    "# in the same way as that of the train data. It also \n",
    "\n",
    "pipe = Pipeline(stages=[opened_by_indexer,location_indexer,category_indexer,priority_indexer,\n",
    "                        assignment_group_indexer,knowledge_indexer,assembler,scaler,svc_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total duration to train the model was around 30 minnutes\n",
    "\n",
    "fit_model=pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a dataframe\n",
    "\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|made_sla_int|prediction|\n",
      "+------------+----------+\n",
      "|           0|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       0.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "|           1|       1.0|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['made_sla_int','prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Area under the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = AUC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the curve is 0.64819823595459\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under the curve is {}\".format(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A roughly 65% area under ROC denotes the model has performed reasonably well in predicting whether an incident has met the sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2. Area under the PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='made_sla_int',metricName='areaUnderPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = PR_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the PR curve is 0.7060097342722701\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under the PR curve is {}\".format(PR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  3. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"made_sla_int\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = ACC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.720598616405478\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of the model is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the confusion matrix: \n",
      " [[ 951 1708]\n",
      " [ 271 4153]]\n"
     ]
    }
   ],
   "source": [
    "y_true = results.select(\"made_sla_int\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = results.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Below is the confusion matrix: \\n {}\".format(cnf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
